{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TXP5nalr9ls"
      },
      "source": [
        "# **My Tokenizer**\n",
        "\n",
        "In this assignment, you are asked to create your own word tokenizer without the help of external tokenizers. Steps to the assignment:\n",
        "1. Choose one of the corpora from nltk.corpus list given - assign it to corpus_name\n",
        "1. Create your tokenizer in the code block - tokenize the selected corpus into token_list\n",
        "1. Give the raw corpus text, corpus_raw, and the my_token_list to the evaluation block\n",
        "\n",
        "Only splitting on whitespace is not enough. At least try two other improvements on the tokenization. Please write sufficient comments to show your reasoning.\n",
        "\n",
        "## Rules\n",
        "### Allowed:\n",
        " - Choosing a top-down tokenizer or bottom-up tokenizer\n",
        " - Using regular expressions library (import re)\n",
        " - Adding additional coding blocks\n",
        " - Having an additional dataset if you are creating a bottom-up tokenizer but you need to be able to run the code standalone.\n",
        "\n",
        "### Not allowed:\n",
        " - Using tokenizer libraries such as nltk.tokenize, or any other external libraries to tokenize.\n",
        " - Changing the contents of the evaluation block at the end of the notebook.\n",
        "\n",
        "## Assignment Report\n",
        "Please write a short assignment report at the end of the notebook (max 500 words). Please include all of the following points in the report:\n",
        " - Corpus name and the selection reason\n",
        " - Design of the tokenizer and reasoning\n",
        " - Challenges you have faced while writing the tokenizer and challenges with the specific corpus\n",
        " - Limitations of your approach\n",
        " - Possible improvements to the system\n",
        "\n",
        "## Grading\n",
        "You will be graded with the following criteria:\n",
        " - running complete code (0.5),\n",
        " - tokenizer algorithm (2),\n",
        " - clear commenting (0.5),\n",
        " - evaluation score - comparison with nltk word tokenizer (at most 1 point),\n",
        " - assignment report (1).\n",
        "\n",
        "## Submission\n",
        "\n",
        "Submission will be made to SUCourse. Please submit your file using the following naming convention.\n",
        "\n",
        "\n",
        "`studentid_studentname_tokenizer.ipynb Â - ex. 26744_aysegulrana_tokenizer.ipynb`\n",
        "\n",
        "\n",
        "**Deadline is October 22nd, 5pm.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "id": "-jYyH_qz3Lii"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def my_tokenizer(corpus_raw):\n",
        "    '''\n",
        "    type corpus_raw: string\n",
        "    param corpus_raw: The raw output of the corpus to be tokenized\n",
        "    rtype: list\n",
        "    return: a list of tokens extracted from the corpus_raw\n",
        "    '''\n",
        "\n",
        "    # write your tokenizer here and apply to corpus_raw. Return the resulting token_list.\n",
        "    # you are NOT allowed to use external tokenizers such as word_tokenize from nltk.\n",
        "    # Only splitting on whitespace is not enough. At least try two other improvements on the tokenization.\n",
        "\n",
        "    # Splitting into lines using endline characters\n",
        "    lines = corpus_raw.split('\\n')\n",
        "\n",
        "    # An empty list for tokens and a list for punctuation characters\n",
        "    words = []\n",
        "    punkt = [\".\", \",\", \"'\", \"!\", \"?\", \";\", \":\", \"$\", \"&\", '\"', \"--\", \"...\", \"-\"]\n",
        "\n",
        "    # Split into sentences\n",
        "    for line in corpus_raw.split('\\n'):\n",
        "\n",
        "        # Get words\n",
        "        for word in line.split():\n",
        "\n",
        "            # Lower the words\n",
        "            word = word.lower()\n",
        "\n",
        "            # Get the punctuations at the end of the sentence and add them to punkt list, they'll be added to the tokens list after the words are processed\n",
        "            temp_punkt_list = []\n",
        "            while word and word[-1] in punkt:\n",
        "                temp_punkt_list.insert(0, word[-1])  # Insert at the beginning of the list\n",
        "                word = word[:-1]\n",
        "\n",
        "            # Remove non-alphanumeric and non-punctuation characters\n",
        "            word = re.sub(r\"[^a-zA-Z0-9\\-.,':;!]\", \"\", word)\n",
        "\n",
        "            # If there is apostrophe, handle it similar to NLTK\n",
        "            if re.search(r\"'\", word):\n",
        "\n",
        "              # If there is a numeric character\n",
        "              if re.search(r\"\\d\", word):\n",
        "\n",
        "                # Handle dates like 87', '99\n",
        "                if re.search(r\"'\\d{2}$\", word):\n",
        "                    words.append(word[:-3].lower())\n",
        "                    words.append(word[-2:])\n",
        "\n",
        "                # Handle decade formats like 1980's\n",
        "                elif re.search(r\"^\\d+'s$\", word):\n",
        "                    words.extend([word[:-2], \"'s\"])\n",
        "\n",
        "              # If there is no numeric character\n",
        "              else:\n",
        "\n",
        "                # Handle clitics\n",
        "\n",
        "                # If it starts with d' or o': !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "                if word[0] == 'o' or word[0] == 'd':\n",
        "                  words.append(word)\n",
        "\n",
        "                # For everything else, just find the apostrophe and split the word from there\n",
        "                else:\n",
        "                  found = re.search(\"'\", word)\n",
        "                  apostrophe_loc = found.start()\n",
        "                  words.append(word[:apostrophe_loc])\n",
        "                  words.append(word[apostrophe_loc:])\n",
        "\n",
        "            # Add the regular words\n",
        "            else:\n",
        "              words.append(word)\n",
        "\n",
        "            # Add punctuations\n",
        "            for punc in temp_punkt_list:\n",
        "              words.append(punc)\n",
        "\n",
        "    token_list = words\n",
        "\n",
        "    return token_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HP0awlu3jci"
      },
      "source": [
        "You are allowed to add code blocks above to use for your tokenizer or evaluate it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZF4WJjKxZfc",
        "outputId": "426b9e85-d3cf-4b7a-9a15-a0433da49be8",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#main code to run your tokenizer.\n",
        "\n",
        "#import your libraries here\n",
        "import nltk\n",
        "\n",
        "#select the corpus name from the list below\n",
        "#gutenberg, webtext, reuters, product_reviews_2\n",
        "\n",
        "corpus_name = \"reuters\"\n",
        "\n",
        "#download the corpus and import it.\n",
        "nltk.download(corpus_name)\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "#get the raw text output of the corpus to the corpus_raw variable.\n",
        "corpus_raw = reuters.raw()\n",
        "\n",
        "#call your tokenizer method\n",
        "my_tokenized_list = my_tokenizer(corpus_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuTDStwY36cT"
      },
      "source": [
        "## Please do not touch the code below that will evaluate your tokenizer with the nltk word tokenizer. You will get zero points from evaluation if you do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "8txzw8Ag8ysD"
      },
      "outputs": [],
      "source": [
        "def similarity_score(set_a, set_b):\n",
        "    '''\n",
        "    type set_a: set\n",
        "    param set_a: The first set to be compared\n",
        "    type set_b: set\n",
        "    param set_b: The tokens extracted from the corpus_raw\n",
        "    rtype: float\n",
        "    return: similarity score with two sets using Jaccard similarity.\n",
        "    '''\n",
        "\n",
        "    jaccard_similarity = float(len(set_a.intersection(set_b)) / len(set_a.union(set_b)))\n",
        "\n",
        "    return jaccard_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wUTqReb36Hg",
        "outputId": "32a42d9d-8ac6-4501-89b8-3e7f0c0cd6aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk import punkt\n",
        "\n",
        "def evaluation(corpus_raw, token_list):\n",
        "    '''\n",
        "    type corpus_raw: string\n",
        "    param corpus_raw: The raw output of the corpus\n",
        "    type token_list: list\n",
        "    param token_list: The tokens extracted from the corpus_raw\n",
        "    rtype: float\"\n",
        "    return: comparison score with the given token list and the nltk tokenizer.\n",
        "    '''\n",
        "\n",
        "    #The comparison score only looks at the tokens but not the frequencies of the tokens.\n",
        "    #we assume case folding is already applied to the token_list\n",
        "    corpus_raw = corpus_raw.lower()\n",
        "    nltk_tokens = word_tokenize(corpus_raw, language='english')\n",
        "\n",
        "    score = similarity_score(set(token_list), set(nltk_tokens))\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Vt_uSBF-NHm",
        "outputId": "c35e8daa-8aec-4c21-df1a-217af4c13259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The similarity score is 0.80\n"
          ]
        }
      ],
      "source": [
        "#Evaluation\n",
        "\n",
        "eval_score = evaluation(corpus_raw, my_tokenized_list)\n",
        "\n",
        "print('The similarity score is {:.2f}'.format(eval_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keZ3UBqh4hgP"
      },
      "source": [
        "Please write your report below using clear headlines with markdown syntax."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRHk0Jos_ymb"
      },
      "source": [
        "# Report\n",
        "\n",
        "## Corpus name and the selection reason\n",
        "\n",
        "I selected Reuters as the corpus because of two reasons:\n",
        "\n",
        "\n",
        "*   I am more interested in working with political data, so I thought this could be a nice start.\n",
        "*   I liked the challenges and conveniences that it will bring, such as not caring about typos but having to fix issues about news-specific expressions.\n",
        "\n",
        "## Design of the tokenizer and reasoning\n",
        "I started with investigating NLTK as it was to be used as a benchmark. I tried several sentences and edge cases and noted how it behaves.\n",
        "\n",
        "I noticed that it has a rather interesting approach to clitics, which was the main focus of my approach.\n",
        "\n",
        "First, I start by splitting the corpus into sentences and words, and I lower the words directly. As it's easier to check the punctuation marks in this stage, I check them and add them to a list that is added to the tokens after the preceding word is handled.\n",
        "\n",
        "After punctuation, I remove non-alphanumeric and non-punctuation characters.\n",
        "\n",
        "The main part starts with checking aposthropes as they are the hardest to handle. I check if it has numerical characters and handle it accordingly, I have two elif statements for this and they are the most common two ways that numbers with apostrophes occur in reuters. If there's no numerical character, i first check if it's a word starts with \"O'\" or \"d'\" and handle accordingly, if not, I just find the aportrophe, split the word from there and add tokens.\n",
        "\n",
        "If it's a regular word, it is also added.\n",
        "\n",
        "Lastly, punctuations are added as mentioned.\n",
        "\n",
        "The reason for this approach was the nature of news oriented dataset and apostrophes being hardest part to handle, therefore I made a tokenizer that puts apostrophes in the center and that has some news-oriented statements around.\n",
        "\n",
        "## Challenges you have faced while writing the tokenizer and challenges with the specific corpus\n",
        "There were some news-specific expressions that were hard to handle.\n",
        "\n",
        "Since there were many dates, I added to separate elif statements just for them and coming up with this idea required some investigation.\n",
        "\n",
        "Another thing was there were some words like \"O'Connor\" and 'd'Angelo\" that originates from other countries, I also added and elif statement to handle these as I noticed while investigating.\n",
        "\n",
        "Corpus has other news specific expressions that were really hard to handle with rule-based approach such as expressions like '\\&lt;national'.\n",
        "\n",
        "## Limitations of your approach\n",
        "As mentioned, the corpus has many interesting cases that applying top down, rule based tokenization is not so easy to. For example, it has expressions like '\\&lt;national' or 'us...nation'. Handling the non-regular punctuations middle of the sentence is really hard and my approach does not work as NLTK does in these cases.\n",
        "\n",
        "## Possible improvements to the system\n",
        "Some code can be added to handle the punctuations in the middle of the words and some news specific expressions like intl'.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
